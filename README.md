# Yardbird

This chicken lays `egg`s...

# Benchmark Results

[Scarecrow](https://scarecrow.sgt-pl.com/)

# Running Example

`cargo run -- --filename examples/array/array_copy.vmt`

# IC3IA

If `ic3ia` binary is located in your system PATH, you can run

```
cargo run -- --filename examples/array/array_copy.vmt --invoke-ic3ia --print-vmt
```

This gives you the IC3IA output on the decorated transition system generated by yardbird.

# Performance Sampling

We've found that using `samply` is a nice way to find out where yardbird is spending time.
Unsurprisingly, it spends a lot of time making Z3 calls, but we've still been able to get some
speedups by looking at the callgraphs graphs and heatmaps.

### If you don't have `samply` installed already

- `cargo install --locked samply`

### Running `yardbird` with `samply`

- `cargo build`

- `samply record ./target/debug/yardbird --filename examples/array/array_copy.vmt`

## Building on Linux

I encountered the following error when building on a Linux machine:

```
error: failed to run custom build command for `z3-sys v0.8.1`

Caused by:
  process didn't exit successfully: `/home/cole/Documents/yardbird/target/debug/build/z3-sys-6ba06f331cb40b8a/build-script-build` (exit status: 101)
  --- stdout
  cargo:rerun-if-changed=build.rs
  cargo:rerun-if-env-changed=Z3_SYS_Z3_HEADER
  cargo:rerun-if-changed=wrapper.h
  cargo:rerun-if-env-changed=TARGET
  cargo:rerun-if-env-changed=BINDGEN_EXTRA_CLANG_ARGS_x86_64-unknown-linux-gnu
  cargo:rerun-if-env-changed=BINDGEN_EXTRA_CLANG_ARGS_x86_64_unknown_linux_gnu
  cargo:rerun-if-env-changed=BINDGEN_EXTRA_CLANG_ARGS

  --- stderr

  thread 'main' panicked at /home/cole/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/bindgen-0.66.1/lib.rs:604:31:
  Unable to find libclang: "couldn't find any valid shared libraries matching: ['libclang.so', 'libclang-*.so', 'libclang.so.*', 'libclang-*.so.*'], set the `LIBCLANG_PATH` environment variable to a path where one of these files can be found (invalid: [])"
  note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace

```

This was resolved by running:

```
sudo apt install libclang-dev
```

# Benchmarking Platform

We've implemented a comprehensive benchmarking platform for systematic evaluation of yardbird across different parameter configurations. The platform supports both local and cloud-based execution with automated result processing and visualization.

## Components

### 1. Configuration System

- **Location**: `garden/benchmark_config.yaml`, `configs/`
- **Purpose**: Define parameter matrices and individual benchmark configurations
- **Features**:
  - Parameter matrices (all combinations of depth, strategy, cost function)
  - Individual configurations for specific tests
  - Global settings for timeout, retry, file patterns
  - Cloud execution settings

### 2. Enhanced Garden CLI

- **Location**: `garden/src/main.rs`
- **Purpose**: Execute benchmarks with configuration files
- **New Features**:
  - Config file support (`--config`)
  - Matrix selection (`--matrix`)
  - Metadata collection (git commit, timestamp)
  - Standardized JSON output schema

### 3. Graphics Generation

- **Location**: `paper-graphics/`
- **Purpose**: Generate publication-ready graphics from benchmark results
- **Components**:
  - `main.py`: Plotly-based scatter plots and analysis
  - `tikz_generator.py`: Raw TikZ code for LaTeX publications
  - Support for both legacy and new JSON formats

### 4. Master CLI Tool

- **Location**: `yardbird-bench`
- **Purpose**: Orchestrate entire benchmark pipeline
- **Features**: Local/EC2 execution, graphics generation, S3 upload

### 5. Cloud Infrastructure

- **Terraform**: `terraform/` - Infrastructure as code for AWS resources

## Usage

### Local Execution

#### Quick Test

```bash
# Build the platform
cargo build --release -p garden

# Run a quick test matrix
./target/release/garden --config configs/quick_test.yaml --matrix fast_test --output results.json

# Generate graphics
cd paper-graphics
uv run main.py ../results.json 5 15
python3 tikz_generator.py ../results.json --all
```

#### Using Master CLI

```bash
# Run benchmarks with automatic graphics generation
./yardbird-bench configs/quick_test.yaml --matrix fast_test --graphics

# Run comprehensive evaluation
./yardbird-bench configs/comprehensive.yaml --matrix full_evaluation --output comprehensive_results.json
```

#### Legacy Mode (Original CLI)

```bash
./target/release/garden examples --strategy abstract --strategy concrete --depth 10 --output legacy_results.json
```

### AWS Cloud Execution

#### Prerequisites

1. Configure AWS credentials (`aws configure`)
2. Create S3 bucket for results
3. Set up EC2 key pair
4. Ensure IAM permissions for EC2 and S3

#### Deploy Infrastructure

```bash
cd terraform

# Initialize and deploy
python3 terraform_runner.py deploy --s3-bucket yardbird-benchmarks --instance-type c5.xlarge

# Or manually with terraform
terraform init
terraform plan -var="s3_bucket_name=yardbird-benchmarks"
terraform apply -var="s3_bucket_name=yardbird-benchmarks"
```

#### Run Cloud Benchmarks

```bash
# Using Terraform runner
python3 terraform_runner.py run configs/paper_evaluation.yaml --matrix paper_main --wait --download ./results/
```

#### Download Results

`source .env && aws s3 sync s3://yardbird-benchmarks-45749081/benchmarks/ benchmark-results`

#### Cleanup Infrastructure

```bash
cd terraform
python3 terraform_runner.py destroy
# or: terraform destroy -auto-approve
```

## Configuration Examples

### Parameter Matrix

```yaml
parameter_matrices:
  comprehensive:
    depths: [10, 15, 20]
    strategies: ["abstract", "concrete"]
    cost_functions: ["symbol-cost", "a-s-t-size"]
    timeout_seconds: 120
```

### Individual Configuration

```yaml
individual_configs:
  - name: "quick_abstract"
    depth: 5
    strategy: "abstract"
    cost_function: "symbol-cost"
    timeout_seconds: 30
```

## Output Formats

### JSON Schema

```json
{
  "metadata": {
    "timestamp": "2025-09-01T14:30:22Z",
    "git_commit": "abc123...",
    "config_name": "paper_main",
    "total_benchmarks": 150,
    "yardbird_version": "0.1.0"
  },
  "benchmarks": [...]
}
```

### Generated Graphics

- **PNG**: Plotly-generated scatter plots and analysis charts
- **TikZ**: Raw LaTeX code for publication graphics
- **Tables**: LaTeX longtable format for benchmark data

## Cost Optimization

- **EC2**: Instances auto-terminate after completion
- **S3**: 90-day lifecycle policy for automatic cleanup
- **Instance Sizing**: c5.xlarge for standard runs, configurable per workload
- **Spot Instances**: Can be configured in Terraform for cost savings

The platform is designed for "fire and forget" cloud execution - launch a run, get results automatically uploaded to S3, and infrastructure tears down to minimize costs.

# notes

measure if first forall requires a model if it takes a long time for MBQI to get a model

just use a trigger based instantiation scheme for insts where we need quantifiers

NO EXPLANATIONS: cargo run -- --filename examples/array/array_copy.vmt -d 300 5.43s user 0.11s system 90% cpu 6.100 total

W EXPLANATIONS: cargo run -- --filename examples/array/array_copy.vmt -d 300 5.27s user 0.17s system 48% cpu 11.230 total
